{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c1f9fa",
   "metadata": {},
   "source": [
    "Load dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "024d1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "# Ensure data and dimensions are correct\n",
    "assert (x_train.shape == (50000, 32, 32, 3))\n",
    "assert (y_train.shape == (50000, 1))\n",
    "assert (x_test.shape == (10000, 32, 32, 3))\n",
    "assert (y_test.shape == (10000,1))\n",
    "\n",
    "# Preprocessing\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "# flatten x_train and x_test arrays\n",
    "# New layout (50000, 32x32x3)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# One-hot encode y_train and y_test\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "10d8e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        # x can be any shape\n",
    "        return sigmoid(x)\n",
    "\n",
    "    def backward(self, a):\n",
    "        # d/dx sigmoid(x) = s * (1 - s)\n",
    "        s = sigmoid(a)\n",
    "        return s * (1.0 - s)\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0.0, x)\n",
    "\n",
    "    def backward(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "class LeakyReLU:\n",
    "    def __init__(self, a=0.01):\n",
    "        # hyperparam a value, so we can tune it for testing\n",
    "        self.a = a\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.maximum(x, self.a * x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return np.where(x > 0, 1.0, self.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2b946c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        #LSE trick\n",
    "        x_stable = np.exp(x - np.max(x, axis = 1, keepdims=True)) #If x is a batch (in the shape of: [batch_size, num_classes]), then np.max(x) would only return a single scalar value, which is the same value subtracted from all rows\n",
    "        exp_sum = np.sum(x_stable, axis=1, keepdims=True)\n",
    "        self.y_pred = x_stable / exp_sum\n",
    "        return self.y_pred\n",
    "\n",
    "    # Simplified dCE_Loss/dSoftMax \n",
    "    def backward(self, y_actual):\n",
    "        return self.y_pred - y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "62e311b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__ (self, drop_rate):\n",
    "        # Noting that the drop_rate resembles the probability that a unit will be set to 0. (e.g. 0.5 for 50%)\n",
    "        self.drop_rate = drop_rate\n",
    "        # Mask will store the indices of the units which are kept (set to 1) during training.\n",
    "        self.mask = None\n",
    "\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the dropout layer\n",
    "\n",
    "        args:\n",
    "            x (np.array) is the input data\n",
    "            training (bool) if true, apply dropout, else return input as is.\n",
    "        \n",
    "        returns:\n",
    "            The output data after applying dropout (if training is set to True), or data is just passing through.\n",
    "        \"\"\"\n",
    "        # Mask is created, 1 (True) to keep the unit at 0 (False) to drop it.\n",
    "        # (1-drop_rate) is the probability of keeping a unit.\n",
    "        if training:\n",
    "            self.mask = np.random.rand(*x.shape) > self.drop_rate\n",
    "            # Multiply the mask by 1/p to maintain the expected value of values.\n",
    "            return x * self.mask*1/(1-self.drop_rate)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Performs the backwards pass for the dropout layer.\n",
    "\n",
    "        args:\n",
    "            d_out (np.array) is the gradient from the subsequent layer\n",
    "\n",
    "        returns:\n",
    "            np.ndarray is the gradient passed to the preceding layer\n",
    "        \"\"\"\n",
    "        # The gradient only flows through the neurons that weren't dropped in the forward pass, and the same inverted scaling factor\n",
    "        # 1/(1-drop_rate) is applied to the gradient.\n",
    "        return d_out * self.mask * 1/(1-self.drop_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d2f2e",
   "metadata": {},
   "source": [
    "Input layer: 3072 nodes<br>\n",
    "Output classification layer: 10 nodes<br><br>To do:<br> Implement L1/L2 Regularisers<br>Look at bottom where training and testing is happening, Add metrics for analysis (for graphs)<br>Add Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3d2f7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    \"\"\"\n",
    "    Callback-style monitor for tracking model performance during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, validation_x=None, validation_y=None):\n",
    "        self.validation_x = validation_x\n",
    "        self.validation_y = validation_y\n",
    "        \n",
    "        # Metric storage using lists\n",
    "        self.metrics = {\n",
    "            'training': {'loss': [], 'accuracy': []},\n",
    "            'validation': {'loss': [], 'accuracy': []}\n",
    "        }\n",
    "    \n",
    "    def has_validation_data(self):\n",
    "        \"\"\"Check if validation dataset is configured.\"\"\"\n",
    "        return self.validation_x is not None and self.validation_y is not None\n",
    "    \n",
    "    def record_epoch(self, epoch_number, loss_value, accuracy_value, phase='training'):\n",
    "        \"\"\"\n",
    "        Store metrics for a completed epoch.\n",
    "        \n",
    "        Args:\n",
    "            epoch_number (int): Current epoch index\n",
    "            loss_value (float): Computed loss\n",
    "            accuracy_value (float): Computed accuracy\n",
    "            phase (str): Either 'training' or 'validation'\n",
    "        \"\"\"\n",
    "        self.metrics[phase]['loss'].append(loss_value)\n",
    "        self.metrics[phase]['accuracy'].append(accuracy_value)\n",
    "    \n",
    "    def log_progress(self, current_epoch, total_epochs):\n",
    "        \"\"\"Print formatted progress update.\"\"\"\n",
    "        train_loss = self.metrics['training']['loss'][-1]\n",
    "        train_acc = self.metrics['training']['accuracy'][-1]\n",
    "        \n",
    "        output = f\"Epoch [{current_epoch + 1}/{total_epochs}] | \"\n",
    "        output += f\"Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\"\n",
    "        \n",
    "        if self.has_validation_data():\n",
    "            val_loss = self.metrics['validation']['loss'][-1]\n",
    "            val_acc = self.metrics['validation']['accuracy'][-1]\n",
    "            output += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "        \n",
    "        print(output)\n",
    "    \n",
    "    def visualize_performance(self):\n",
    "        \"\"\"Generate performance visualization plots.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        num_epochs = len(self.metrics['training']['loss'])\n",
    "        epoch_numbers = np.arange(1, num_epochs + 1)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss visualization\n",
    "        ax1.plot(epoch_numbers, self.metrics['training']['loss'], \n",
    "                marker='o', markersize=3, linewidth=2.5, color='#2E86AB', label='Training')\n",
    "        if self.has_validation_data():\n",
    "            ax1.plot(epoch_numbers, self.metrics['validation']['loss'], \n",
    "                    marker='s', markersize=3, linewidth=2.5, color='#E63946', label='Validation')\n",
    "        ax1.set_xlabel('Epoch Number', fontsize=13)\n",
    "        ax1.set_ylabel('Cross-Entropy Loss', fontsize=13)\n",
    "        ax1.set_title('Loss Progression', fontsize=15, fontweight='bold', pad=15)\n",
    "        ax1.legend(loc='best', framealpha=0.9)\n",
    "        ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "        \n",
    "        # Accuracy visualization\n",
    "        ax2.plot(epoch_numbers, self.metrics['training']['accuracy'], \n",
    "                marker='o', markersize=3, linewidth=2.5, color='#2E86AB', label='Training')\n",
    "        if self.has_validation_data():\n",
    "            ax2.plot(epoch_numbers, self.metrics['validation']['accuracy'], \n",
    "                    marker='s', markersize=3, linewidth=2.5, color='#E63946', label='Validation')\n",
    "        ax2.set_xlabel('Epoch Number', fontsize=13)\n",
    "        ax2.set_ylabel('Classification Accuracy', fontsize=13)\n",
    "        ax2.set_title('Accuracy Progression', fontsize=15, fontweight='bold', pad=15)\n",
    "        ax2.legend(loc='best', framealpha=0.9)\n",
    "        ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def create_data_splits(x_full, y_full, train_size, val_size):\n",
    "    \"\"\"\n",
    "    Utility function to partition dataset into train/validation subsets.\n",
    "    \n",
    "    Args:\n",
    "        x_full (np.ndarray): Complete feature dataset\n",
    "        y_full (np.ndarray): Complete label dataset\n",
    "        train_size (int): Number of training samples\n",
    "        val_size (int): Number of validation samples\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (x_train, y_train, x_val, y_val)\n",
    "    \"\"\"\n",
    "    x_train = x_full[:train_size]\n",
    "    y_train = y_full[:train_size]\n",
    "    \n",
    "    x_val = x_full[train_size:train_size + val_size]\n",
    "    y_val = y_full[train_size:train_size + val_size]\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "391f1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, x_train, y_train, hidden_layers, hidden_layer_sizes, activation_func, optimiser, learning_rate=0.03, dropout_rate=0.5):\n",
    "        \n",
    "        self.inputs = x_train\n",
    "        self.outputs = y_train\n",
    "\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        if (len(hidden_layer_sizes) != hidden_layers):\n",
    "            raise ValueError(\"Neurons array length mismatch with hidden layers amount\")\n",
    "        self.hidden_layers_sizes = hidden_layer_sizes\n",
    "\n",
    "        if not isinstance(activation_func, (Sigmoid, ReLU, LeakyReLU)):\n",
    "            raise TypeError(\"Activation function must be of type Sigmoid, ReLU or LeakyReLU\")\n",
    "        self.activation_func = activation_func\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weights, self.biases = self.initalise_parameters()\n",
    "\n",
    "        self.activations = [None] * (self.hidden_layers + 2)\n",
    "\n",
    "        # Dropout instance per hidden layer\n",
    "        self.dropout_layers = [Dropout(dropout_rate) for _ in range(self.hidden_layers)]\n",
    "        \n",
    "        # Might remove - recheck\n",
    "        self.dropout_outputs = [None] * self.hidden_layers\n",
    "\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "        # For Optimisers\n",
    "        self.optimiser = optimiser    \n",
    "        # RMS_Prop\n",
    "        if optimiser == \"RMS_Prop\":\n",
    "            self.r_weights, self.r_biases = self.create_zeroed_matrix_per_parameter()\n",
    "        # Adam\n",
    "        if optimiser == \"Adam\":\n",
    "            self.adam_moment1_weights, self.adam_moment1_biases = self.create_zeroed_matrix_per_parameter()\n",
    "            self.adam_moment2_weights, self.adam_moment2_biases = self.create_zeroed_matrix_per_parameter()\n",
    "            self.adam_timestep = 0\n",
    "\n",
    "\n",
    "    def initalise_parameters(self):\n",
    "        network_layout = [len(self.inputs[0])]\n",
    "        for i in range(self.hidden_layers):\n",
    "            network_layout.append(self.hidden_layers_sizes[i])\n",
    "        network_layout.append(len(self.outputs[0]))\n",
    "        \n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        # Sigmoid weight initialisation\n",
    "        if isinstance(self.activation_func, Sigmoid):\n",
    "            for i in range(len(network_layout) -  1):\n",
    "                input = network_layout[i]\n",
    "                output = network_layout[i+1]\n",
    "                weight_init = (np.random.uniform(low=-1, high=1, size=(output, input))) / (np.sqrt(input))\n",
    "                bias = np.zeros(output)\n",
    "                weights.append(weight_init)\n",
    "                biases.append(bias)\n",
    "        \n",
    "        # ReLU weight initialisation\n",
    "        else:\n",
    "            for i in range(len(network_layout) -  1):\n",
    "                input = network_layout[i]\n",
    "                output = network_layout[i+1]\n",
    "                # Better He initialization\n",
    "                weight_init = np.random.randn(output, input) * np.sqrt(2.0 / input)\n",
    "                bias = np.zeros(output)\n",
    "                weights.append(weight_init)\n",
    "                biases.append(bias)\n",
    "\n",
    "        return weights, biases\n",
    "    \n",
    "    \n",
    "     # Creates empty matrices for weights/bias needed for the optimisers\n",
    "    def create_zeroed_matrix_per_parameter(self):\n",
    "        weights_velocity = []\n",
    "        biases_velocity = []\n",
    "        for i in range(len(self.weights)):\n",
    "            weights_velocity.append(np.zeros(self.weights[i].shape))\n",
    "            biases_velocity.append(np.zeros(self.biases[i].shape))\n",
    "        return weights_velocity, biases_velocity\n",
    "    \n",
    "    \n",
    "    def train(self, epochs, batch_size=64, monitor=None, lr_decay_factor=0.95, lr_decay_every=10, early_stop_patience=10):\n",
    "        \"\"\"\n",
    "        Execute training loop with optional performance tracking.\n",
    "        \n",
    "        Args:\n",
    "            epochs (int): Total number of training iterations\n",
    "            batch_size (int): Number of samples per gradient update\n",
    "            monitor (TrainingMonitor, optional): Callback for tracking metrics\n",
    "            lr_decay_factor (float): Factor to multiply learning rate by (default: 0.95)\n",
    "            lr_decay_every (int): Apply decay every N epochs (default: 10)\n",
    "            early_stop_patience (int): Stop if val loss doesn't improve for N epochs (default: 10)\n",
    "            \n",
    "        Returns:\n",
    "            TrainingMonitor: Object containing complete training statistics\n",
    "        \"\"\"\n",
    "        if monitor is None:\n",
    "            monitor = TrainingMonitor()\n",
    "        \n",
    "        # Early stopping tracking\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        initial_lr = self.learning_rate\n",
    "        \n",
    "        for epoch_idx in range(epochs):\n",
    "            # Learning rate decay\n",
    "            if epoch_idx > 0 and epoch_idx % lr_decay_every == 0:\n",
    "                self.learning_rate *= lr_decay_factor\n",
    "                print(f\"Learning rate decayed to: {self.learning_rate:.6f}\")\n",
    "            \n",
    "            # Randomize training order each iteration\n",
    "            shuffled_x, shuffled_y = self.shuffle_dataset()\n",
    "\n",
    "            # Initialize epoch statistics\n",
    "            cumulative_loss = 0.0\n",
    "            num_batches = 0\n",
    "            total_correct = 0\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for batch_start in range(0, len(self.inputs), batch_size):\n",
    "                batch_end = batch_start + batch_size\n",
    "                x_batch = shuffled_x[batch_start:batch_end]\n",
    "                y_batch = shuffled_y[batch_start:batch_end]\n",
    "\n",
    "                # Forward propagation\n",
    "                _ = self.forwardpass(x_batch, training=True)\n",
    "\n",
    "                # Calculate loss\n",
    "                batch_loss = self.cross_entropy_loss(self.softmax.y_pred, y_batch)\n",
    "\n",
    "                # Backpropagation\n",
    "                grads_w, grads_b = self.backprop(_, y_batch, batch_size)\n",
    "\n",
    "                # Parameter update\n",
    "                self.update_weights(grads_w, grads_b)\n",
    "                \n",
    "                # Aggregate batch statistics\n",
    "                num_batches += 1\n",
    "                cumulative_loss += batch_loss\n",
    "                predictions = np.argmax(self.softmax.y_pred, axis=1)\n",
    "                targets = np.argmax(y_batch, axis=1)\n",
    "                total_correct += np.sum(predictions == targets)\n",
    "            \n",
    "            # Calculate epoch-level metrics\n",
    "            avg_loss = cumulative_loss / num_batches\n",
    "            accuracy = total_correct / len(self.inputs)\n",
    "            \n",
    "            # Update monitor with training results\n",
    "            monitor.record_epoch(epoch_idx, avg_loss, accuracy, phase='training')\n",
    "            \n",
    "            # Evaluate on hold-out set if available\n",
    "            if monitor.has_validation_data():\n",
    "                val_loss, val_accuracy = self._evaluate_performance(monitor.validation_x, monitor.validation_y)\n",
    "                monitor.record_epoch(epoch_idx, val_loss, val_accuracy, phase='validation')\n",
    "                \n",
    "                # Early stopping check\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= early_stop_patience:\n",
    "                        print(f\"\\nEarly stopping triggered at epoch {epoch_idx + 1}\")\n",
    "                        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                        break\n",
    "            \n",
    "            # Display progress\n",
    "            monitor.log_progress(epoch_idx, epochs)\n",
    "        \n",
    "        return monitor \n",
    "\n",
    "    \n",
    "    def forwardpass(self, batch_x, training=True):\n",
    "        cache = []\n",
    "        self.activations[0] = batch_x \n",
    "        for j in range(self.hidden_layers + 1):\n",
    "            \n",
    "            # Recheck this block and usage of self.dropout_outputs if necessary\n",
    "            if j==0:\n",
    "                input_to_layer = self.activations[j]\n",
    "            else:\n",
    "                input_to_layer = self.dropout_outputs[j-1]\n",
    "\n",
    "            \n",
    "            z1 = np.dot(input_to_layer, self.weights[j].T) + self.biases[j]\n",
    "                \n",
    "            # Softmax\n",
    "            if j == (len(self.weights) - 1):\n",
    "                self.softmax.forward(z1)\n",
    "                self.activations[j+1] = self.softmax.y_pred\n",
    "                cache.append(z1)\n",
    "                cache.append(self.softmax.y_pred)\n",
    "\n",
    "            # All other layers\n",
    "            else:\n",
    "                a1 = self.activation_func.forward(z1)\n",
    "                \n",
    "                # Integrated Dropout\n",
    "                # Recheck this block and usage of self.dropout_outputs if necessary\n",
    "                if training:\n",
    "                    a_dropout = self.dropout_layers[j].forward(a1, training=True)\n",
    "                else:\n",
    "                    a_dropout = a1\n",
    "\n",
    "                self.activations[j+1] = a1\n",
    "                self.dropout_outputs[j] = a_dropout\n",
    "                cache.append(z1)\n",
    "                cache.append(a1)\n",
    "            \n",
    "        return cache\n",
    "\n",
    "    \n",
    "    def backprop(self, cache, batch_y, batch_size=64):\n",
    "        weight_gradients = []\n",
    "        bias_gradients = []\n",
    "        \n",
    "        # Softmax layer backprop\n",
    "        dsMax = self.softmax.backward(batch_y)\n",
    "        dw_sMax = np.dot(dsMax.T, self.activations[-2])\n",
    "        db_sMax = np.sum(dsMax, axis=0)\n",
    "\n",
    "        # Average over batch size\n",
    "        dw_sMax /= batch_size\n",
    "        db_sMax /= batch_size\n",
    "\n",
    "        # Store gradients in weight and bias lists\n",
    "        weight_gradients.append(dw_sMax)\n",
    "        bias_gradients.append(db_sMax)\n",
    "        \n",
    "        # Running gradient across entire network\n",
    "        grad = dsMax\n",
    "\n",
    "        j = len(self.weights) - 2\n",
    "        for i in range(len(cache) - 4, -1, -2):\n",
    "            z = cache[i]\n",
    "            a = cache[i+1]\n",
    "            prev_a = cache[i-1] if i > 0 else self.activations[0]\n",
    "\n",
    "            da = np.dot(grad, self.weights[j+1])\n",
    "            \n",
    "            # Recheck this block and usage of self.dropout_outputs if necessary\n",
    "            da = self.dropout_layers[j].backward(da)\n",
    "            \n",
    "            dz = da * self.activation_func.backward(z)\n",
    "            \n",
    "            dw = np.dot(dz.T, prev_a) / batch_size\n",
    "            weight_gradients.append(dw)\n",
    "\n",
    "            db = np.sum(dz, axis=0) / batch_size\n",
    "            bias_gradients.append(db)\n",
    "            \n",
    "            grad = dz\n",
    "            j -= 1\n",
    "\n",
    "        weight_gradients.reverse()\n",
    "        bias_gradients.reverse()\n",
    "        return weight_gradients, bias_gradients\n",
    "    \n",
    "    \n",
    "    def update_weights(self, weight_gradients, bias_gradients):\n",
    "        if self.optimiser == \"None\":\n",
    "            self.vanille_gradient_descent(weight_gradients, bias_gradients)\n",
    "        elif self.optimiser == \"RMS_Prop\":\n",
    "            self.RMS_Prop(weight_gradients, bias_gradients)\n",
    "        elif self.optimiser == \"Adam\":\n",
    "            self.adam(weight_gradients, bias_gradients)\n",
    "        else:\n",
    "            raise ValueError(\"Optimiser should be either: None, RMS_Prop or Adam\")\n",
    "\n",
    "    \n",
    "    # Vanilla Mini-Batch GD \n",
    "    def vanille_gradient_descent(self, weight_gradients, bias_gradients):\n",
    "        for i in range(len(weight_gradients)):\n",
    "            self.weights[i] = self.weights[i] - (self.learning_rate * weight_gradients[i])\n",
    "            self.biases[i] = self.biases[i] - (self.learning_rate * bias_gradients[i])\n",
    "        return self.weights\n",
    "\n",
    "    \n",
    "    # Optimiser 1 - RMSProp\n",
    "    # decay rate hyperparam can be tuned but 0.9 is stable\n",
    "    def RMS_Prop(self, weight_gradients, bias_gradients, decay_rate=0.9, epsilon=1e-8):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.r_weights[i] =  (decay_rate * self.r_weights[i]) + ((1 - decay_rate) * (weight_gradients[i] ** 2))\n",
    "            self.r_biases[i] =  (decay_rate * self.r_biases[i]) + ((1 - decay_rate) * (bias_gradients[i] ** 2))\n",
    "            self.weights[i] -= (self.learning_rate * weight_gradients[i]) / np.sqrt(self.r_weights[i] + epsilon)\n",
    "            self.biases[i] -= (self.learning_rate * bias_gradients[i]) / np.sqrt(self.r_biases[i] + epsilon)\n",
    "        return self.weights\n",
    "\n",
    "    \n",
    "    \n",
    "    # Optimiser 2 - Adam\n",
    "    def adam(self, weight_gradients, bias_gradients, momentum_Beta=0.9, RMS_Prop_decay=0.999, epsilon=1e-8):\n",
    "        self.adam_timestep += 1\n",
    "        # Uses Momentum and RMSProp\n",
    "        for i in range(len(self.weights)):\n",
    "            # Moment 1 - Momentum style\n",
    "            self.adam_moment1_weights[i] = (momentum_Beta * self.adam_moment1_weights[i]) + ((1 - momentum_Beta) * weight_gradients[i])\n",
    "            self.adam_moment1_biases[i] = (momentum_Beta * self.adam_moment1_biases[i]) + ((1 - momentum_Beta) * bias_gradients[i])\n",
    "            \n",
    "            # Moment 2 - RMS_Prop style\n",
    "            self.adam_moment2_weights[i] = (RMS_Prop_decay * self.adam_moment2_weights[i]) + ((1 - RMS_Prop_decay) * (weight_gradients[i] ** 2))\n",
    "            self.adam_moment2_biases[i] = (RMS_Prop_decay * self.adam_moment2_biases[i]) + ((1 - RMS_Prop_decay) * (bias_gradients[i] ** 2))\n",
    "            \n",
    "            # Use timestep, which is incremented per batch to correct biases\n",
    "            moment1_weights_bias_corrected = self.adam_moment1_weights[i] / (1 - (momentum_Beta ** self.adam_timestep))\n",
    "            moment1_biases_bias_corrected = self.adam_moment1_biases[i] / (1 - (momentum_Beta ** self.adam_timestep))\n",
    "            moment2_weights_bias_corrected = self.adam_moment2_weights[i] / (1 - (RMS_Prop_decay ** self.adam_timestep))\n",
    "            moment2_biases_bias_corrected = self.adam_moment2_biases[i] / (1 - (RMS_Prop_decay ** self.adam_timestep))\n",
    "            # Actual update\n",
    "            self.weights[i] = self.weights[i] - ((self.learning_rate * moment1_weights_bias_corrected) / np.sqrt(moment2_weights_bias_corrected + epsilon))\n",
    "            self.biases[i] = self.biases[i] - ((self.learning_rate * moment1_biases_bias_corrected) / np.sqrt(moment2_biases_bias_corrected + epsilon))\n",
    "        return self.weights\n",
    "\n",
    "    \n",
    "    \n",
    "    # Runs the test data (not part of training)\n",
    "    def run(self, x_test):\n",
    "        original_inputs = self.inputs\n",
    "        self.inputs = x_test\n",
    "        # Ignore cache\n",
    "        cache = self.forwardpass(x_test, training=False)\n",
    "        # Creates 1d array where highest probability index = predicted class\n",
    "        y_pred = np.argmax(self.softmax.y_pred, axis=1)\n",
    "\n",
    "        self.inputs = original_inputs\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    \n",
    "    def cross_entropy_loss(self, y_pred, y_true, epsilon=1e-8):\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(y_true * np.log(y_pred), axis=1)\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Shuffle dataset for Mini-Batch GD, so it is representative of entire dataset\n",
    "    def shuffle_dataset(self):\n",
    "        random_indices = np.random.permutation(len(self.inputs)) # Set of random indices between 0 and end of dataset \n",
    "        x_shuffled = self.inputs[random_indices]\n",
    "        y_shuffled = self.outputs[random_indices]\n",
    "        return x_shuffled, y_shuffled\n",
    "    \n",
    "    \n",
    "    def _evaluate_performance(self, features, labels):\n",
    "        \"\"\"\n",
    "        Compute loss and accuracy on a given dataset.\n",
    "        \n",
    "        Args:\n",
    "            features (np.ndarray): Input data\n",
    "            labels (np.ndarray): Ground truth labels (one-hot)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (loss, accuracy)\n",
    "        \"\"\"\n",
    "        _ = self.forwardpass(features, training=False)\n",
    "        loss = self.cross_entropy_loss(self.softmax.y_pred, labels)\n",
    "        predictions = np.argmax(self.softmax.y_pred, axis=1)\n",
    "        targets = np.argmax(labels, axis=1)\n",
    "        accuracy = np.mean(predictions == targets)\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f8ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Training set: 35000 samples\n",
      "  Validation set: 5000 samples\n",
      "  Test set: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED TRAINING CONFIGURATION with improvements\n",
    "# Partition dataset into training and validation subsets\n",
    "x_train_subset, y_train_subset, x_val_subset, y_val_subset = create_data_splits(\n",
    "    x_train, y_train, \n",
    "    train_size=35000,  # Training samples (70% of available data)\n",
    "    val_size=5000      # Validation set\n",
    ")\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Training set: {x_train_subset.shape[0]} samples\")\n",
    "print(f\"  Validation set: {x_val_subset.shape[0]} samples\")\n",
    "print(f\"  Test set: {x_test.shape[0]} samples\")\n",
    "\n",
    "# Initialize network with optimized settings\n",
    "np.random.seed(20)\n",
    "network_fast = NeuralNetwork(\n",
    "    x_train_subset,\n",
    "    y_train_subset,\n",
    "    hidden_layers=4,\n",
    "    hidden_layer_sizes=[1024, 512, 256, 128],\n",
    "    activation_func=LeakyReLU(),\n",
    "    optimiser=\"Adam\",\n",
    "    learning_rate=0.001,\n",
    "    dropout_rate=0.05\n",
    ")\n",
    "\n",
    "# Create monitor with validation data\n",
    "performance_tracker_fast = TrainingMonitor(\n",
    "    validation_x=x_val_subset,\n",
    "    validation_y=y_val_subset\n",
    ")\n",
    "\n",
    "monitor_results_fast = network_fast.train(\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    monitor=performance_tracker_fast,\n",
    "    lr_decay_factor=0.95,\n",
    "    lr_decay_every=10,\n",
    "    early_stop_patience=999  \n",
    ")\n",
    "\n",
    "monitor_results_fast.visualize_performance()\n",
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c668441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 4735 / 10000\n",
      "Test Accuracy: 47.35%\n"
     ]
    }
   ],
   "source": [
    "# Test the Neural Network (using the trained model)\n",
    "y_pred = network_fast.run(x_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "# Convert into boolean array and then sum over all true elements\n",
    "correct_predictions = np.sum((y_pred == y_true))\n",
    "accuracy = correct_predictions / len(y_true)\n",
    "print(f\"Correct predictions: {correct_predictions} / {len(y_true)}\")\n",
    "print(f\"Test Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
