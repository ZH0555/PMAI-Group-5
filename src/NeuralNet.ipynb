{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c1f9fa",
   "metadata": {},
   "source": [
    "Load dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024d1972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "(50000, 3072)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "# Ensure data and dimensions are correct\n",
    "assert (x_train.shape == (50000, 32, 32, 3))\n",
    "assert (y_train.shape == (50000, 1))\n",
    "assert (x_test.shape == (10000, 32, 32, 3))\n",
    "assert (y_test.shape == (10000,1))\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "# Min-Max scale normalisation,returns values between 0-1\n",
    "x_train = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))\n",
    "# Change type to 4 byte float for performance if needed, currently 8 byte float\n",
    "print(type(x_train[0][0][0][0]))\n",
    "\n",
    "# flatten x_train and x_test arrays\n",
    "# New layout (50000, 32x32x3)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "print(x_train.shape)\n",
    "\n",
    "# One-hot encode y_train and y_test\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d8e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x can be any shape\n",
    "        self.out = sigmoid(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # d/dx sigmoid(x) = s * (1 - s)\n",
    "        return dout * self.out * (1.0 - self.out)\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = np.maximum(0.0, x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout.copy()\n",
    "        dx[self.out <= 0.0] = 0.0\n",
    "        return dx\n",
    "    \n",
    "class LeakyReLU:\n",
    "    def __init__(self, a):\n",
    "        self.out = None\n",
    "        # hyperparam a value, so we can tune it for testing\n",
    "        self.a = a\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = np.maximum(x, self.a * x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return np.where(dout >= 0, 1.0, self.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b946c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # May be overflow for larger numbers so potentially use different method\n",
    "        exp_sum = np.sum(np.exp(x))\n",
    "        self.y_pred = np.exp(x) / sum\n",
    "        return self.y_pred\n",
    "\n",
    "    # Using Cross-Entropy-Loss \n",
    "    def backward(self, y_actual):\n",
    "        return self.y_pred - y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e311b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout:\n",
    "    def __init__ (self, drop_rate):\n",
    "        # Noting that the drop_rate resembles the probability that a unit will be set to 0. (e.g. 0.5 for 50%)\n",
    "        self.drop_rate = drop_rate\n",
    "        # Mask will store the indices of the units which are kept (set to 1) during training.\n",
    "        self.mask = None\n",
    "\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the dropout layer\n",
    "\n",
    "        args:\n",
    "            x (np.array) is the input data\n",
    "            training (bool) if true, apply dropout, else return input as is.\n",
    "        \n",
    "        returns:\n",
    "            The output data after applying dropout (if training is set to True), or data is just passing through.\n",
    "        \"\"\"\n",
    "        # Mask is created, 1 (True) to keep the unit at 0 (False) to drop it.\n",
    "        # (1-drop_rate) is the probability of keeping a unit.\n",
    "        if training:\n",
    "            self.mask = np.random.rand(*x.shape) > self.drop_rate\n",
    "            # Multiply the mask by 1/p to maintain the expected value of values.\n",
    "            return x * self.mask*1/(1-self.drop_rate)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Performs the backwards pass for the dropout layer.\n",
    "\n",
    "        args:\n",
    "            d_out (np.array) is the gradient from the subsequent layer\n",
    "\n",
    "        returns:\n",
    "            np.ndarray is the gradient passed to the preceding layer\n",
    "        \"\"\"\n",
    "        # The gradient only flows through the neurons that weren't dropped in the forward pass, and the same inverted scaling factor\n",
    "        # 1/(1-drop_rate) is applied to the gradient.\n",
    "        return d_out * self.mask * 1/(1-self.drop_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d2f2e",
   "metadata": {},
   "source": [
    "Input layer: 3072 nodes<br>\n",
    "Output classification layer: 10 nodes<br><br>To do:<br> Simplify anything and there may be bugs<br> Implement L1/L2 Regularisers<br>Activation Functions should be chosen per layer, currently it's 1 for the entire network<br>Look at bottom where training and testing is happening, Add metrics for analysis (for graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LayerCache:\n",
    "    \"\"\"Structured storage for layer computations during forward/backward passes.\"\"\"\n",
    "    def __init__(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, z, activation, pre_dropout=None):\n",
    "        \"\"\"Store layer computation results.\"\"\"\n",
    "        self.layers.append({\n",
    "            'z': z,\n",
    "            'activation': activation,\n",
    "            'pre_dropout': pre_dropout if pre_dropout is not None else activation\n",
    "        })\n",
    "    \n",
    "    def get_layer(self, idx):\n",
    "        \"\"\"Retrieve layer data by index.\"\"\"\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def get_previous_activation(self, layer_idx):\n",
    "        \"\"\"Get the activation from the previous layer, or input if first layer.\"\"\"\n",
    "        if layer_idx == 0:\n",
    "            return self.input\n",
    "        return self.layers[layer_idx - 1]['activation']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x_train, y_train, hidden_layers, hidden_layer_sizes, activation_func, optimiser, learning_rate=0.03):\n",
    "        \n",
    "        self.inputs = x_train\n",
    "        self.outputs = y_train\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        if len(hidden_layer_sizes) != hidden_layers:\n",
    "            raise ValueError(\"Neurons array length mismatch with hidden layers amount\")\n",
    "        self.hidden_layers_sizes = hidden_layer_sizes\n",
    "\n",
    "        if not isinstance(activation_func, (Sigmoid, ReLU, LeakyReLU)):\n",
    "            raise TypeError(\"Activation function must be of type Sigmoid, ReLU or LeakyReLU\")\n",
    "        self.activation_func = activation_func\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize network architecture\n",
    "        self.num_layers = hidden_layers + 1  # hidden layers + output layer\n",
    "        self.weights, self.biases = self._initialize_parameters()\n",
    "\n",
    "        # Layer components\n",
    "        self.dropout_layers = [Dropout(drop_rate=0.5) for _ in range(hidden_layers)]\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "        # Optimizer state\n",
    "        self.optimiser = optimiser\n",
    "        if optimiser in [\"RMS_Prop\", \"Adam\"]:\n",
    "            self.r_weights, self.r_biases = self._create_zeroed_matrices()\n",
    "        if optimiser == \"Adam\":\n",
    "            self.adam_m1_weights, self.adam_m1_biases = self._create_zeroed_matrices()\n",
    "            self.adam_m2_weights, self.adam_m2_biases = self._create_zeroed_matrices()\n",
    "            self.adam_timestep = 0\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize weights and biases using appropriate strategy.\"\"\"\n",
    "        layer_sizes = [self.inputs.shape[1]] + self.hidden_layers_sizes + [self.outputs.shape[1]]\n",
    "        weights, biases = [], []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            fan_in, fan_out = layer_sizes[i], layer_sizes[i + 1]\n",
    "            \n",
    "            # Xavier/He initialization\n",
    "            if isinstance(self.activation_func, Sigmoid):\n",
    "                scale = np.sqrt(1.0 / fan_in)  # Xavier\n",
    "            else:  # ReLU, LeakyReLU\n",
    "                scale = np.sqrt(2.0 / fan_in)  # He initialization\n",
    "            \n",
    "            W = np.random.uniform(-1, 1, (fan_out, fan_in)) * scale\n",
    "            b = np.zeros(fan_out)\n",
    "            \n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def _create_zeroed_matrices(self):\n",
    "        \"\"\"Create zero-initialized matrices matching parameter shapes.\"\"\"\n",
    "        weights_zeros = [np.zeros_like(W) for W in self.weights]\n",
    "        biases_zeros = [np.zeros_like(b) for b in self.biases]\n",
    "        return weights_zeros, biases_zeros\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        \"\"\"Train the network for specified number of epochs.\"\"\"\n",
    "        loss_history = []\n",
    "        batch_size = 64\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            x_shuffled, y_shuffled = self._shuffle_dataset()\n",
    "            epoch_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_start in range(0, len(self.inputs), batch_size):\n",
    "                batch_x = x_shuffled[batch_start:batch_start + batch_size]\n",
    "                batch_y = y_shuffled[batch_start:batch_start + batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                cache = self.forwardpass(batch_x, training=True)\n",
    "                loss = self._cross_entropy_loss(self.softmax.y_pred, batch_y)\n",
    "                \n",
    "                # Backward pass\n",
    "                weight_grads, bias_grads = self._backprop(cache, batch_y, len(batch_x))\n",
    "                \n",
    "                # Update parameters\n",
    "                self._update_parameters(weight_grads, bias_grads)\n",
    "                \n",
    "                epoch_loss += loss\n",
    "                num_batches += 1\n",
    "            \n",
    "            loss_history.append(epoch_loss / num_batches)\n",
    "        \n",
    "        return loss_history\n",
    "\n",
    "    def forwardpass(self, batch_x, training=True):\n",
    "        \"\"\"\n",
    "        Forward propagation through all layers.\n",
    "        \n",
    "        Args:\n",
    "            batch_x: Input batch of shape (batch_size, input_dim)\n",
    "            training: Whether to apply dropout\n",
    "        \n",
    "        Returns:\n",
    "            LayerCache: Structured cache containing all layer computations\n",
    "        \"\"\"\n",
    "        cache = LayerCache(batch_x)\n",
    "        current_input = batch_x\n",
    "        \n",
    "        # Process hidden layers\n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = current_input @ self.weights[i].T + self.biases[i]\n",
    "            activation = self.activation_func.forward(z)\n",
    "            \n",
    "            # Apply dropout if training\n",
    "            pre_dropout = activation\n",
    "            if training:\n",
    "                activation = self.dropout_layers[i].forward(activation, training)\n",
    "            \n",
    "            cache.add_layer(z, activation, pre_dropout)\n",
    "            current_input = activation\n",
    "        \n",
    "        # Output layer with softmax\n",
    "        z_out = current_input @ self.weights[-1].T + self.biases[-1]\n",
    "        self.softmax.forward(z_out)\n",
    "        output = self.softmax.y_pred\n",
    "        cache.add_layer(z_out, output)\n",
    "        \n",
    "        return cache\n",
    "\n",
    "    def _backprop(self, cache, y_true, batch_size):\n",
    "        \"\"\"\n",
    "        Backpropagation through all layers.\n",
    "        \n",
    "        Args:\n",
    "            cache: LayerCache from forward pass\n",
    "            y_true: True labels\n",
    "            batch_size: Size of current batch\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (weight_gradients, bias_gradients)\n",
    "        \"\"\"\n",
    "        weight_grads = []\n",
    "        bias_grads = []\n",
    "        \n",
    "        # Start with output layer gradient from softmax\n",
    "        grad = self.softmax.backward(y_true)\n",
    "        \n",
    "        # Backprop through all layers in reverse\n",
    "        for layer_idx in range(self.num_layers - 1, -1, -1):\n",
    "            layer_data = cache.get_layer(layer_idx)\n",
    "            prev_activation = cache.get_previous_activation(layer_idx)\n",
    "            \n",
    "            # Compute parameter gradients for this layer\n",
    "            dW = prev_activation.T @ grad / batch_size\n",
    "            db = np.sum(grad, axis=0) / batch_size\n",
    "            \n",
    "            weight_grads.insert(0, dW)\n",
    "            bias_grads.insert(0, db)\n",
    "            \n",
    "            # Propagate gradient to previous layer (if not at input)\n",
    "            if layer_idx > 0:\n",
    "                # Gradient w.r.t. input of this layer\n",
    "                grad = grad @ self.weights[layer_idx]\n",
    "                \n",
    "                # Apply activation function derivative for the PREVIOUS layer (layer we're propagating to)\n",
    "                # Always use 'z' for activation backward, regardless of activation type\n",
    "                prev_layer = cache.get_layer(layer_idx - 1)\n",
    "                grad = grad * self.activation_func.backward(prev_layer['z'])\n",
    "                \n",
    "                # Apply dropout backward (only for hidden layers, not output)\n",
    "                if layer_idx - 1 < len(self.dropout_layers):\n",
    "                    grad = self.dropout_layers[layer_idx - 1].backward(grad)\n",
    "        \n",
    "        return weight_grads, bias_grads\n",
    "\n",
    "    def _update_parameters(self, weight_grads, bias_grads):\n",
    "        \"\"\"Update parameters using the selected optimizer.\"\"\"\n",
    "        if self.optimiser == \"None\":\n",
    "            self._sgd_update(weight_grads, bias_grads)\n",
    "        elif self.optimiser == \"RMS_Prop\":\n",
    "            self._rmsprop_update(weight_grads, bias_grads)\n",
    "        elif self.optimiser == \"Adam\":\n",
    "            self._adam_update(weight_grads, bias_grads)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.optimiser}\")\n",
    "\n",
    "    def _sgd_update(self, weight_grads, bias_grads):\n",
    "        \"\"\"Standard SGD parameter update.\"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * weight_grads[i].T\n",
    "            self.biases[i] -= self.learning_rate * bias_grads[i]\n",
    "\n",
    "    def _rmsprop_update(self, weight_grads, bias_grads):\n",
    "        \"\"\"RMSProp optimizer update.\"\"\"\n",
    "        beta = 0.999\n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            self.r_weights[i] = beta * self.r_weights[i] + (1 - beta) * np.square(weight_grads[i].T)\n",
    "            self.weights[i] -= self.learning_rate * weight_grads[i].T / (np.sqrt(self.r_weights[i]) + epsilon)\n",
    "            \n",
    "            self.r_biases[i] = beta * self.r_biases[i] + (1 - beta) * np.square(bias_grads[i])\n",
    "            self.biases[i] -= self.learning_rate * bias_grads[i] / (np.sqrt(self.r_biases[i]) + epsilon)\n",
    "\n",
    "    def _adam_update(self, weight_grads, bias_grads):\n",
    "        \"\"\"Adam optimizer update.\"\"\"\n",
    "        beta1, beta2 = 0.9, 0.999\n",
    "        epsilon = 1e-8\n",
    "        self.adam_timestep += 1\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            # Weight updates\n",
    "            self.adam_m1_weights[i] = beta1 * self.adam_m1_weights[i] + (1 - beta1) * weight_grads[i].T\n",
    "            self.adam_m2_weights[i] = beta2 * self.adam_m2_weights[i] + (1 - beta2) * np.square(weight_grads[i].T)\n",
    "            \n",
    "            # Bias correction\n",
    "            m1_corrected = self.adam_m1_weights[i] / (1 - beta1 ** self.adam_timestep)\n",
    "            m2_corrected = self.adam_m2_weights[i] / (1 - beta2 ** self.adam_timestep)\n",
    "            \n",
    "            self.weights[i] -= self.learning_rate * m1_corrected / (np.sqrt(m2_corrected) + epsilon)\n",
    "            \n",
    "            # Bias updates\n",
    "            self.adam_m1_biases[i] = beta1 * self.adam_m1_biases[i] + (1 - beta1) * bias_grads[i]\n",
    "            self.adam_m2_biases[i] = beta2 * self.adam_m2_biases[i] + (1 - beta2) * np.square(bias_grads[i])\n",
    "            \n",
    "            m1_corrected = self.adam_m1_biases[i] / (1 - beta1 ** self.adam_timestep)\n",
    "            m2_corrected = self.adam_m2_biases[i] / (1 - beta2 ** self.adam_timestep)\n",
    "            \n",
    "            self.biases[i] -= self.learning_rate * m1_corrected / (np.sqrt(m2_corrected) + epsilon)\n",
    "\n",
    "    def _cross_entropy_loss(self, y_pred, y_true):\n",
    "        \"\"\"Compute cross-entropy loss with numerical stability.\"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred_clipped), axis=1))\n",
    "\n",
    "    def _shuffle_dataset(self):\n",
    "        \"\"\"Shuffle training data.\"\"\"\n",
    "        indices = np.random.permutation(len(self.inputs))\n",
    "        return self.inputs[indices], self.outputs[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ea372",
   "metadata": {},
   "source": [
    "Basic Training, Add more advanced metrics for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd30e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Neural Network\n",
    "relu = ReLU()\n",
    "np.random.seed(20) # Set seed so results are reproducible across runs\n",
    "nn = NeuralNetwork(x_train, y_train, 3, [5,5,5], relu, \"Adam\")\n",
    "loss_history = nn.train(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf476108",
   "metadata": {},
   "source": [
    "Basic Testing, Add more advanced metrics for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c668441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Neural Network\n",
    "y_pred = nn.forwardpass(x_test, training=False)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "# Convert into boolean array and then sum over all true elements\n",
    "correct_predictions = np.sum((y_pred == y_true))\n",
    "accuracy = correct_predictions / len(y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
