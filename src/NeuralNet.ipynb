{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d8e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x can be any shape\n",
    "        self.out = sigmoid(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # d/dx sigmoid(x) = s * (1 - s)\n",
    "        return dout * self.out * (1.0 - self.out)\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = np.maximum(0.0, x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout.copy()\n",
    "        dx[self.out <= 0.0] = 0.0\n",
    "        return dx\n",
    "    \n",
    "class LeakyReLU:\n",
    "    def __init__(self, a):\n",
    "        self.out = None\n",
    "        # hyperparam a value, so we can tune it for testing\n",
    "        self.a = a\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = np.maximum(x, self.a * x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return np.where(dout >= 0, 1.0, self.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b946c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # May be overflow for larger numbers so potentially use different method\n",
    "        exp_sum = np.sum(np.exp(x))\n",
    "        self.y_pred = np.exp(x) / sum\n",
    "        return self.y_pred\n",
    "\n",
    "    # Using Cross-Entropy-Loss \n",
    "    def backward(self, y_actual):\n",
    "        return self.y_pred - y_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d2f2e",
   "metadata": {},
   "source": [
    "Input layer: 3072 nodes<br>\n",
    "Output classification layer: 10 nodes<br><br>Note: add biases<br>simplify anything if necessary<br> implement dropout<br>implement optimisers<br>Maybe change activations to be per layer not entire network if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x_train, y_train, hidden_layers, hidden_layer_sizes, activation_func,learning_rate=0.03):\n",
    "        \n",
    "        self.inputs = x_train\n",
    "        self.outputs = y_train\n",
    "\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        if (len(hidden_layer_sizes) != hidden_layers):\n",
    "            raise ValueError(\"Neurons array length mismatch with hidden layers amount\")\n",
    "        self.hidden_layers_sizes = hidden_layer_sizes\n",
    "\n",
    "        if not isinstance(activation_func, (Sigmoid, ReLU, LeakyReLU)):\n",
    "            raise TypeError(\"Activation function must be of type Sigmoid, ReLU or LeakyReLU\")\n",
    "        self.activation_func = activation_func\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weights = self.create_weight_matrices()\n",
    "\n",
    "        self.activations = [None] * (self.hidden_layers + 2)\n",
    "\n",
    "    def create_weight_matrices(self):\n",
    "        network_layout = [len(self.inputs[0])]\n",
    "        for i in range(self.hidden_layers):\n",
    "            network_layout.append(self.hidden_layers_sizes[i])\n",
    "        network_layout.append(len(self.outputs[0]))\n",
    "        \n",
    "        weights = []\n",
    "\n",
    "        # Sigmoid weight initialisation\n",
    "        if isinstance(self.activation_func, Sigmoid):\n",
    "            for i in range(len(network_layout) -  1):\n",
    "                input = network_layout[i]\n",
    "                output = network_layout[i+1]\n",
    "                weight_init = (np.random.uniform(low=-1, high=1, size=(output, input))) / (np.sqrt(input))\n",
    "                weights.append(weight_init)\n",
    "        \n",
    "        # ReLU weight initialisation\n",
    "        if isinstance(self.activation_func, (ReLU, LeakyReLU)):\n",
    "            for i in range(len(network_layout) -  1):\n",
    "                input = network_layout[i]\n",
    "                output = network_layout[i+1]\n",
    "                weight_init = (np.random.uniform(low=-1, high=1, size=(output, input))) / (np.sqrt(input) / 2)\n",
    "                weights.append(weight_init)\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    def train(self, epochs=100):\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            # basic forward pass\n",
    "            loss_history = []\n",
    "            loss, cache, softmax = self.forwardpass()            \n",
    "            loss_history.append(loss)\n",
    "\n",
    "            gradients = self.backprop(cache, softmax)\n",
    "            self.weights = self.update_gradients(gradients)\n",
    "        pass \n",
    "\n",
    "    def forwardpass(self):\n",
    "        cache = []\n",
    "        self.activations[0] = self.inputs\n",
    "        softmax = Softmax()\n",
    "        for j in range(len(self.weights)):\n",
    "            z1 = np.dot(self.activations[j], self.weights[j].T)\n",
    "                \n",
    "            # Softmax\n",
    "            if j == len(self.weights) - 1:\n",
    "                softmax.forward(z1)\n",
    "                self.activations[j+1] = softmax.y_pred\n",
    "                cache.append(z1)\n",
    "                cache.append(softmax.y_pred)    \n",
    "            # All other layers\n",
    "            else:\n",
    "                a1 = self.activation_func.forward(z1)\n",
    "                self.activations[j+1] = a1\n",
    "                cache.append(z1)\n",
    "                cache.append(a1)\n",
    "            \n",
    "        # Compute CE_loss\n",
    "        ce_loss = self.cross_entropy_loss(softmax.y_pred, self.outputs)\n",
    "        return ce_loss, cache, softmax\n",
    "\n",
    "    def backprop(self, cache, softmax):\n",
    "        gradients = []\n",
    "        dsMax = softmax.backward(self.outputs)\n",
    "        dw_sMax = np.dot((self.activations[-2]).T, dsMax)\n",
    "        gradients.append(dw_sMax)\n",
    "        grad = dsMax\n",
    "        j = len(self.weights) - 2\n",
    "        for i in range(len(cache) - 4, -1, -2):\n",
    "            z = cache[i]\n",
    "            a = cache[i+1]\n",
    "            prev_a = cache[i-1]\n",
    "            da = np.dot(self.weights[j].T, grad)\n",
    "            if isinstance (self.activations, (ReLU, LeakyReLU)):\n",
    "                dz = da * self.activations.backward(z)\n",
    "            else:\n",
    "                dz = da * self.activations.backward(a)\n",
    "            dw = np.dot(prev_a.T, dz)\n",
    "            gradients.append(dw)\n",
    "            grad = dz\n",
    "            j -= 1\n",
    "        return gradients.reverse()\n",
    "    \n",
    "    def update_gradients():\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "    def cross_entropy_loss(self, y_pred, y_actual):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(y_actual * np.log(y_pred), axis=1)\n",
    "        return np.mean(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
