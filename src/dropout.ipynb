{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e272009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout:\n",
    "    def __init__ (self, drop_rate):\n",
    "        # Noting that the drop_rate resembles the probability that a unit will be set to 0. (e.g. 0.5 for 50%)\n",
    "        self.drop_rate = drop_rate\n",
    "        # Mask will store the indices of the units which are kept (set to 1) during training.\n",
    "        self.mask = None\n",
    "\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the dropout layer\n",
    "\n",
    "        args:\n",
    "            x (np.array) is the input data\n",
    "            training (bool) if true, apply dropout, else return input as is.\n",
    "        \n",
    "        returns:\n",
    "            The output data after applying dropout (if training is set to True), or data is just passing through.\n",
    "        \"\"\"\n",
    "        # Mask is created, 1 (True) to keep the unit at 0 (False) to drop it.\n",
    "        # (1-drop_rate) is the probability of keeping a unit.\n",
    "        if training:\n",
    "            self.mask = np.random.rand(*x.shape) > self.drop_rate\n",
    "            # Multiply the mask by 1/p to maintain the expected value of values.\n",
    "            return x * self.mask*1/(1-self.drop_rate)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Performs the backwards pass for the dropout layer.\n",
    "\n",
    "        args:\n",
    "            d_out (np.array) is the gradient from the subsequent layer\n",
    "\n",
    "        returns:\n",
    "            np.ndarray is the gradient passed to the preceding layer\n",
    "        \"\"\"\n",
    "        # The gradient only flows through the neurons that weren't dropped in the forward pass, and the same inverted scaling factor\n",
    "        # 1/(1-drop_rate) is applied to the gradient.\n",
    "        return d_out * self.mask * 1/(1-self.drop_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
